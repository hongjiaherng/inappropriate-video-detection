{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41323fe0-5bf7-4982-adbc-676f5815d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, Literal\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as tv_transforms\n",
    "import torchvision.transforms.v2.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c88ccd70-9d2b-47d9-a79c-d155373b994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reload module to access .py files easily\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(\"../src/\")\n",
    "if not src_path in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import transforms as custom_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cff0a571-587d-4aab-8d5c-572c16619b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hf_dataset(\n",
    "    hf_dataset_name: str, progress: Set[str]\n",
    ") -> datasets.IterableDataset:\n",
    "    \"\"\"\n",
    "    Initialize HuggingFace dataset (both train and test splits) and filter out videos that have already been processed.\n",
    "    Note: Currently only supports streaming huggingface datasets but not non-streaming huggingface dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    if hf_dataset_name == \"jherng/xd-violence\":\n",
    "\n",
    "        def extract_relative_dir(full_filepath: str):\n",
    "            data_url = \"/datasets/jherng/xd-violence/resolve/main/data/video\"\n",
    "            return \"/\".join(\n",
    "                urlsplit(full_filepath)\n",
    "                .path.split(data_url)[-1]\n",
    "                .lstrip(\"/\")\n",
    "                .split(\"/\")[:-1]  # relative_dir\n",
    "            )\n",
    "\n",
    "        train_ds = datasets.load_dataset(\n",
    "            hf_dataset_name, name=\"video\", split=\"train\", streaming=True\n",
    "        ).map(\n",
    "            remove_columns=[\n",
    "                \"binary_target\",\n",
    "                \"multilabel_targets\",\n",
    "                \"frame_annotations\",\n",
    "            ]\n",
    "        )  # Remove unused columns for preprocessing\n",
    "\n",
    "        test_ds = datasets.load_dataset(\n",
    "            hf_dataset_name, name=\"video\", split=\"test\", streaming=True\n",
    "        ).map(\n",
    "            remove_columns=[\n",
    "                \"binary_target\",\n",
    "                \"multilabel_targets\",\n",
    "                \"frame_annotations\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Concatenate train and test datasets\n",
    "        combined_ds = datasets.concatenate_datasets([train_ds, test_ds])\n",
    "\n",
    "        # Filter out videos that have already been processed\n",
    "        # assume there's always a subdir in the path at 2nd last position,\n",
    "        # e.g., 1-1004 from https://huggingface.co/datasets/.../1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4\n",
    "        combined_ds = combined_ds.filter(\n",
    "            lambda x: \"/\".join([extract_relative_dir(x[\"path\"]), x[\"id\"]])\n",
    "            not in progress\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {hf_dataset_name} not supported. Currently only supports ['jherng/xd-violence'].\"\n",
    "        )\n",
    "\n",
    "    return combined_ds, extract_relative_dir\n",
    "\n",
    "preprocessing_cfg = dict(\n",
    "    io_backend=None,  # to be supplied by upstream\n",
    "    id_key=None,  # to be supplied by upstream\n",
    "    path_key=None,  # to be supplied by upstream\n",
    "    num_clips=None,  # to be supplied by upstream\n",
    "    crop_type=None,  # to be supplied by upstream\n",
    "    clip_len=32,\n",
    "    sampling_rate=2,\n",
    "    resize_size=256,\n",
    "    crop_size=224,\n",
    "    mean=(0.485, 0.456, 0.406),\n",
    "    std=(0.229, 0.224, 0.225),\n",
    ")\n",
    "\n",
    "def build_preprocessing_pipeline(\n",
    "    io_backend: Literal[\"http\", \"local\"],\n",
    "    id_key: str = \"id\",\n",
    "    path_key: str = \"path\",\n",
    "    num_clips: int = -1,\n",
    "    crop_type: Literal[\"10-crop\", \"5-crop\", \"center\"] = \"5-crop\",\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Takes in a whole video and returns a tensor of shape (num_clips, num_crops, num_channels, clip_len, crop_h, crop_w) = (num_clips, num_crops, 3, 32, 224, 224).\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessing_cfg[\"io_backend\"] = io_backend\n",
    "    preprocessing_cfg[\"id_key\"] = id_key\n",
    "    preprocessing_cfg[\"path_key\"] = path_key\n",
    "    preprocessing_cfg[\"num_clips\"] = num_clips\n",
    "    preprocessing_cfg[\"crop_type\"] = crop_type\n",
    "\n",
    "    crop_type_config = {\n",
    "        \"5-crop\": custom_transforms.FiveCrop,\n",
    "        \"10-crop\": custom_transforms.TenCrop,\n",
    "        \"center\": custom_transforms.CenterCrop,\n",
    "    }\n",
    "\n",
    "    pipeline = [\n",
    "        custom_transforms.AdaptDataFormat(\n",
    "            id_key=preprocessing_cfg[\"id_key\"],\n",
    "            path_key=preprocessing_cfg[\"path_key\"],\n",
    "        ),\n",
    "        custom_transforms.VideoReaderInit(io_backend=preprocessing_cfg[\"io_backend\"]),\n",
    "        custom_transforms.TemporalClipSample(\n",
    "            clip_len=preprocessing_cfg[\"clip_len\"],\n",
    "            sampling_rate=preprocessing_cfg[\"sampling_rate\"],\n",
    "            num_clips=preprocessing_cfg[\"num_clips\"],\n",
    "        ),\n",
    "        custom_transforms.VideoDecode(),\n",
    "        custom_transforms.Resize(size=preprocessing_cfg[\"resize_size\"]),\n",
    "        crop_type_config[preprocessing_cfg[\"crop_type\"]](\n",
    "            size=preprocessing_cfg[\"crop_size\"]\n",
    "        ),\n",
    "        custom_transforms.ToDType(dtype=torch.float32, scale=True),\n",
    "        custom_transforms.Normalize(\n",
    "            mean=preprocessing_cfg[\"mean\"], std=preprocessing_cfg[\"std\"]\n",
    "        ),\n",
    "        custom_transforms.ConvertTCHWToCTHW(lead_dims=2),\n",
    "        custom_transforms.PackInputs(preserved_meta=[\"id\", \"filename\"]),\n",
    "    ]\n",
    "\n",
    "    return tv_transforms.Compose(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "089d0e64-5b11-457f-b0f1-5db78ed496a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<datasets.iterable_dataset.IterableDataset at 0x26d497ae230>,\n",
       " Compose(\n",
       "       AdaptDataFormat(id_key=id, path_key=path)\n",
       "       VideoReaderInit(io_backend=http)\n",
       "       TemporalClipSample(clip_len=32, num_clips=-1, sampling_rate=2)\n",
       "       VideoDecode()\n",
       "       Resize(size=256)\n",
       "       FiveCrop(size=224)\n",
       "       ToDType(dtype=torch.float32, scale=True)\n",
       "       Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "       ConvertTCHWToCTHW(lead_dims=2)\n",
       "       PackInputs(preserved_meta=['id', 'filename'])\n",
       " ))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset, _ = init_hf_dataset(\"jherng/xd-violence\", progress=set())\n",
    "preprocessing = build_preprocessing_pipeline(io_backend=\"http\", num_clips=-1)\n",
    "\n",
    "hf_dataset, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ed9a9-dca9-4f68-b46c-3327a0218104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'id': 'A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A', 'path': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4'}\n"
     ]
    }
   ],
   "source": [
    "for i, video_ex in enumerate(hf_dataset):\n",
    "    print(i)\n",
    "    print(video_ex)\n",
    "\n",
    "    video_ex = preprocessing(video_ex)\n",
    "    \n",
    "    print(video_ex[\"meta\"])\n",
    "    print(video_ex[\"inputs\"].size())\n",
    "\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7e6f1-b680-455a-8784-02bea4f73884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-env",
   "language": "python",
   "name": "fyp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
