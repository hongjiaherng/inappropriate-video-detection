{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41323fe0-5bf7-4982-adbc-676f5815d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jia Herng\\miniconda3\\envs\\fyp-env\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\Jia Herng\\miniconda3\\envs\\fyp-env\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from typing import Set, Literal\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as tv_transforms\n",
    "import torchvision.transforms.v2.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88ccd70-9d2b-47d9-a79c-d155373b994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload module to access .py files easily\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(\"../src/\")\n",
    "if not src_path in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import transforms as custom_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff0a571-587d-4aab-8d5c-572c16619b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hf_dataset(\n",
    "    hf_dataset_name: str, progress: Set[str]\n",
    ") -> datasets.IterableDataset:\n",
    "    \"\"\"\n",
    "    Initialize HuggingFace dataset (both train and test splits) and filter out videos that have already been processed.\n",
    "    Note: Currently only supports streaming huggingface datasets but not non-streaming huggingface dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    if hf_dataset_name == \"jherng/xd-violence\":\n",
    "\n",
    "        def extract_relative_dir(full_filepath: str):\n",
    "            data_url = \"/datasets/jherng/xd-violence/resolve/main/data/video\"\n",
    "            return \"/\".join(\n",
    "                urlsplit(full_filepath)\n",
    "                .path.split(data_url)[-1]\n",
    "                .lstrip(\"/\")\n",
    "                .split(\"/\")[:-1]  # relative_dir\n",
    "            )\n",
    "\n",
    "        train_ds = datasets.load_dataset(\n",
    "            hf_dataset_name, name=\"video\", split=\"train\", streaming=True\n",
    "        ).map(\n",
    "            remove_columns=[\n",
    "                \"binary_target\",\n",
    "                \"multilabel_targets\",\n",
    "                \"frame_annotations\",\n",
    "            ]\n",
    "        )  # Remove unused columns for preprocessing\n",
    "\n",
    "        test_ds = datasets.load_dataset(\n",
    "            hf_dataset_name, name=\"video\", split=\"test\", streaming=True\n",
    "        ).map(\n",
    "            remove_columns=[\n",
    "                \"binary_target\",\n",
    "                \"multilabel_targets\",\n",
    "                \"frame_annotations\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Concatenate train and test datasets\n",
    "        combined_ds = datasets.concatenate_datasets([train_ds, test_ds])\n",
    "\n",
    "        # Filter out videos that have already been processed\n",
    "        # assume there's always a subdir in the path at 2nd last position,\n",
    "        # e.g., 1-1004 from https://huggingface.co/datasets/.../1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4\n",
    "        combined_ds = combined_ds.filter(\n",
    "            lambda x: \"/\".join([extract_relative_dir(x[\"path\"]), x[\"id\"]])\n",
    "            not in progress\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {hf_dataset_name} not supported. Currently only supports ['jherng/xd-violence'].\"\n",
    "        )\n",
    "\n",
    "    return combined_ds, extract_relative_dir\n",
    "\n",
    "preprocessing_cfg = dict(\n",
    "    io_backend=None,  # to be supplied by upstream\n",
    "    id_key=None,  # to be supplied by upstream\n",
    "    path_key=None,  # to be supplied by upstream\n",
    "    num_clips=None,  # to be supplied by upstream\n",
    "    crop_type=None,  # to be supplied by upstream\n",
    "    clip_len=32,\n",
    "    sampling_rate=2,\n",
    "    resize_size=256,\n",
    "    crop_size=224,\n",
    "    mean=(0.485, 0.456, 0.406),\n",
    "    std=(0.229, 0.224, 0.225),\n",
    ")\n",
    "\n",
    "def build_preprocessing_pipeline(\n",
    "    io_backend: Literal[\"http\", \"local\"],\n",
    "    id_key: str = \"id\",\n",
    "    path_key: str = \"path\",\n",
    "    num_clips: int = -1,\n",
    "    crop_type: Literal[\"10-crop\", \"5-crop\", \"center\"] = \"5-crop\",\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Takes in a whole video and returns a tensor of shape (num_clips, num_crops, num_channels, clip_len, crop_h, crop_w) = (num_clips, num_crops, 3, 32, 224, 224).\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessing_cfg[\"io_backend\"] = io_backend\n",
    "    preprocessing_cfg[\"id_key\"] = id_key\n",
    "    preprocessing_cfg[\"path_key\"] = path_key\n",
    "    preprocessing_cfg[\"num_clips\"] = num_clips\n",
    "    preprocessing_cfg[\"crop_type\"] = crop_type\n",
    "\n",
    "    crop_type_config = {\n",
    "        \"5-crop\": custom_transforms.FiveCrop,\n",
    "        \"10-crop\": custom_transforms.TenCrop,\n",
    "        \"center\": custom_transforms.CenterCrop,\n",
    "    }\n",
    "\n",
    "    pipeline = [\n",
    "        custom_transforms.AdaptDataFormat(\n",
    "            id_key=preprocessing_cfg[\"id_key\"],\n",
    "            path_key=preprocessing_cfg[\"path_key\"],\n",
    "        ),\n",
    "        custom_transforms.VideoReaderInit(io_backend=preprocessing_cfg[\"io_backend\"]),\n",
    "        custom_transforms.TemporalClipSample(\n",
    "            clip_len=preprocessing_cfg[\"clip_len\"],\n",
    "            sampling_rate=preprocessing_cfg[\"sampling_rate\"],\n",
    "            num_clips=preprocessing_cfg[\"num_clips\"],\n",
    "        ),\n",
    "        custom_transforms.VideoDecode(),\n",
    "        custom_transforms.Resize(size=preprocessing_cfg[\"resize_size\"]),\n",
    "        crop_type_config[preprocessing_cfg[\"crop_type\"]](\n",
    "            size=preprocessing_cfg[\"crop_size\"]\n",
    "        ),\n",
    "        custom_transforms.ToDType(dtype=torch.float32, scale=True),\n",
    "        custom_transforms.Normalize(\n",
    "            mean=preprocessing_cfg[\"mean\"], std=preprocessing_cfg[\"std\"]\n",
    "        ),\n",
    "        custom_transforms.ConvertTCHWToCTHW(lead_dims=2),\n",
    "        custom_transforms.PackInputs(preserved_meta=[\"id\", \"filename\"]),\n",
    "    ]\n",
    "\n",
    "    return tv_transforms.Compose(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089d0e64-5b11-457f-b0f1-5db78ed496a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<datasets.iterable_dataset.IterableDataset at 0x1fb72e0a410>,\n",
       " Compose(\n",
       "       AdaptDataFormat(id_key=id, path_key=path)\n",
       "       VideoReaderInit(io_backend=http)\n",
       "       TemporalClipSample(clip_len=32, num_clips=-1, sampling_rate=2)\n",
       "       VideoDecode()\n",
       "       Resize(size=256)\n",
       "       FiveCrop(size=224)\n",
       "       ToDType(dtype=torch.float32, scale=True)\n",
       "       Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "       ConvertTCHWToCTHW(lead_dims=2)\n",
       "       PackInputs(preserved_meta=['id', 'filename'])\n",
       " ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset, _ = init_hf_dataset(\"jherng/xd-violence\", progress=set())\n",
    "preprocessing = build_preprocessing_pipeline(io_backend=\"http\", num_clips=-1)\n",
    "\n",
    "hf_dataset, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038ed9a9-dca9-4f68-b46c-3327a0218104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'id': 'A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A', 'path': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4'}\n",
      "{'id': 'A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A', 'filename': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4'}\n",
      "torch.Size([24, 5, 3, 32, 224, 224])\n",
      "1\n",
      "{'id': 'A.Beautiful.Mind.2001__#00-03-00_00-04-05_label_A', 'path': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-03-00_00-04-05_label_A.mp4'}\n",
      "{'id': 'A.Beautiful.Mind.2001__#00-03-00_00-04-05_label_A', 'filename': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-03-00_00-04-05_label_A.mp4'}\n",
      "torch.Size([24, 5, 3, 32, 224, 224])\n",
      "2\n",
      "{'id': 'A.Beautiful.Mind.2001__#00-04-20_00-05-35_label_A', 'path': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-04-20_00-05-35_label_A.mp4'}\n",
      "{'id': 'A.Beautiful.Mind.2001__#00-04-20_00-05-35_label_A', 'filename': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-04-20_00-05-35_label_A.mp4'}\n",
      "torch.Size([28, 5, 3, 32, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i, video_ex in enumerate(hf_dataset):\n",
    "    print(i)\n",
    "    print(video_ex)\n",
    "\n",
    "    video_ex = preprocessing(video_ex)\n",
    "    \n",
    "    print(video_ex[\"meta\"])\n",
    "    print(video_ex[\"inputs\"].size())\n",
    "\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe7e6f1-b680-455a-8784-02bea4f73884",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_data = custom_transforms.AdaptDataFormat(id_key=\"id\", path_key=\"path\")\n",
    "video_reader_init = custom_transforms.VideoReaderInit(io_backend=\"http\")\n",
    "temporal_clip_sample = custom_transforms.TemporalClipSample(clip_len=32, sampling_rate=2, num_clips=-1)\n",
    "video_decode = custom_transforms.VideoDecode()\n",
    "resize = custom_transforms.Resize(size=256)\n",
    "crop = custom_transforms.FiveCrop(size=224)\n",
    "to_dtype = custom_transforms.ToDType(dtype=torch.float32, scale=True)\n",
    "normalize = custom_transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "convert_tchw2cthw = custom_transforms.ConvertTCHWToCTHW(lead_dims=2)\n",
    "pack_inputs = custom_transforms.PackInputs(preserved_meta=[\"id\", \"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b22d787-5edf-4ebf-8481-dfc03a481083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A', 'path': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4'}\n",
      "torch.Size([24, 32, 3, 346, 640])\n",
      "torch.Size([24, 32, 3, 256, 473])\n",
      "torch.Size([24, 5, 32, 3, 224, 224])\n",
      "torch.Size([24, 5, 3, 32, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i, video_ex in enumerate(hf_dataset):\n",
    "    if i == 1:\n",
    "        break\n",
    "    print(video_ex)\n",
    "    video_ex = adapt_data(video_ex)\n",
    "    video_ex = video_reader_init(video_ex)\n",
    "    video_ex = temporal_clip_sample(video_ex)\n",
    "    video_ex = video_decode(video_ex)\n",
    "\n",
    "    print(video_ex[\"inputs\"].shape)\n",
    "    \n",
    "    video_ex = resize(video_ex)\n",
    "    print(video_ex[\"inputs\"].shape)\n",
    "    \n",
    "    video_ex = crop(video_ex)\n",
    "    print(video_ex[\"inputs\"].shape)\n",
    "    \n",
    "    video_ex = to_dtype(video_ex)\n",
    "    video_ex = normalize(video_ex)\n",
    "    video_ex = convert_tchw2cthw(video_ex)\n",
    "    print(video_ex[\"inputs\"].shape)\n",
    "    \n",
    "    video_ex = pack_inputs(video_ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4b00443-5e83-4a08-b16f-ad2ba3351201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04dc110f-a679-43eb-9478-83cc84251399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " {'id': 'A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A', 'path': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4'}\n",
      "\n",
      "output['meta']:\n",
      " {'id': 'A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A', 'filename': 'https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4', 'avg_fps': 24.0, 'num_clips': 24, 'clip_len': 32, 'sampling_rate': 2, 'frame_shape': (224, 224), 'num_crops': 5}\n",
      "\n",
      "output['inputs'].shape:\n",
      " torch.Size([24, 5, 3, 32, 224, 224])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as tv_transforms\n",
    "import transforms as custom_transforms\n",
    "\n",
    "example = next(iter(hf_dataset))\n",
    "\n",
    "pipeline = tv_transforms.Compose([\n",
    "    custom_transforms.AdaptDataFormat(id_key=\"id\", path_key=\"path\"),\n",
    "    custom_transforms.VideoReaderInit(io_backend=\"http\"),\n",
    "    custom_transforms.TemporalClipSample(clip_len=32, sampling_rate=2, num_clips=-1),\n",
    "    custom_transforms.VideoDecode(),\n",
    "    custom_transforms.Resize(size=256),\n",
    "    custom_transforms.FiveCrop(size=224),\n",
    "    custom_transforms.ToDType(dtype=torch.float32, scale=True),\n",
    "    custom_transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    custom_transforms.ConvertTCHWToCTHW(lead_dims=2),\n",
    "    custom_transforms.PackInputs(preserved_meta=[\n",
    "        \"id\", \"filename\", \"num_crops\", \"frame_shape\", \"clip_len\", \n",
    "        \"num_clips\", \"sampling_rate\", \"avg_fps\"\n",
    "    ]),\n",
    "])\n",
    "\n",
    "output = pipeline(example)\n",
    "\n",
    "print(\"input:\\n\", example, end=\"\\n\\n\")\n",
    "print(\"output['meta']:\\n\", output[\"meta\"], end=\"\\n\\n\")\n",
    "print(\"output['inputs'].shape:\\n\", output[\"inputs\"].shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e7755e1-93c6-452d-a5d1-90c5a5aea1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      "(24, 5, 2048)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"features:\")\n",
    "print(np.load(\"C:/Users/Jia Herng/Documents/Jia Herng's Docs/Final Year Project/inappropriate-video-detection/feature-extractor/data/outputs/i3d_rgb/1-1004/A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.npy\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40887d-748e-4668-8aab-bbf583c5a957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-env",
   "language": "python",
   "name": "fyp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
