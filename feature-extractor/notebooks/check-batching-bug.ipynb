{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20cf2a92-949f-4eb2-85d5-91e30d653180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reload module to access .py files easily\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "\n",
    "import torchvision.transforms.v2 as tv_transforms\n",
    "    \n",
    "src_path = os.path.abspath(\"../src/\")\n",
    "if not src_path in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import transforms as my_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7094ab3e-97fd-4bfa-be64-69eaa142ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently differnt batch size yield different feature extraction results, which is not expected.\n",
    "# First, investigate the equality of video inputs tensor with different batch size.\n",
    "# Second, investigate the equality of feature extraction results with different batch size.\n",
    "video_ex = {\n",
    "    \"id\": \"A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A\",\n",
    "    \"path\": \"https://huggingface.co/datasets/jherng/xd-violence/resolve/main/data/video/1-1004/A.Beautiful.Mind.2001__%2300-01-45_00-02-50_label_A.mp4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db35ae2d-88b1-4a5d-876f-e516cc860956",
   "metadata": {},
   "source": [
    "## 1. First, investigate the equality of video inputs tensor with different batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31ccad1-0143-4a3f-b7a2-7c8532aeb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# into tensor\n",
    "video2batched = tv_transforms.Compose([\n",
    "    my_transforms.AdaptDataFormat(id_key=\"id\", path_key=\"path\"),\n",
    "    my_transforms.VideoReaderInit(io_backend=\"http\"),\n",
    "    my_transforms.TemporalClipSample(\n",
    "        clip_len=32,\n",
    "        sampling_rate=2,\n",
    "        num_clips=-1,\n",
    "    ),\n",
    "    my_transforms.ClipBatching(batch_size=4),\n",
    "    my_transforms.BatchDecodeIter(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d4d722-69b7-45b4-8069-df16ea8b7310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 32, 3, 346, 640])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_clips = []\n",
    "for i, batch in enumerate(video2batched(video_ex)):\n",
    "    batched_clips.append(batch[\"inputs\"])\n",
    "batched_clips = torch.concatenate(batched_clips, axis=0)\n",
    "batched_clips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38901ee1-93e6-4cba-ba4a-3d4b2d427fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# into tensor\n",
    "video2full = tv_transforms.Compose([\n",
    "    my_transforms.AdaptDataFormat(id_key=\"id\", path_key=\"path\"),\n",
    "    my_transforms.VideoReaderInit(io_backend=\"http\"),\n",
    "    my_transforms.TemporalClipSample(\n",
    "        clip_len=32,\n",
    "        sampling_rate=2,\n",
    "        num_clips=-1,\n",
    "    ),\n",
    "    my_transforms.VideoDecode()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "062ec5d4-d992-4d0c-94c8-1b6c81130516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 32, 3, 346, 640])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_clips = video2full(video_ex)[\"inputs\"]\n",
    "full_clips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956d0a0f-17f5-4693-bfc0-ec54fd09eddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(batched_clips, full_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee57105-1351-41e9-bfbd-ec8701cf2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# into tensor\n",
    "video2batched2 = tv_transforms.Compose([\n",
    "    my_transforms.AdaptDataFormat(id_key=\"id\", path_key=\"path\"),\n",
    "    my_transforms.VideoReaderInit(io_backend=\"http\"),\n",
    "    my_transforms.TemporalClipSample(\n",
    "        clip_len=32,\n",
    "        sampling_rate=2,\n",
    "        num_clips=-1,\n",
    "    ),\n",
    "    my_transforms.ClipBatching(batch_size=8),\n",
    "    my_transforms.BatchDecodeIter(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d56863-1c18-47a0-9c1a-f1672b9a9a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 32, 3, 346, 640])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_clips2 = []\n",
    "for i, batch in enumerate(video2batched2(video_ex)):\n",
    "    batched_clips2.append(batch[\"inputs\"])\n",
    "batched_clips2 = torch.concatenate(batched_clips2, axis=0)\n",
    "batched_clips2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99284c64-1a8d-4f7b-a338-efda11fca6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(batched_clips2, batched_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf97491-be1e-45a1-9ee8-42d7dea0029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_pipe = tv_transforms.Compose([\n",
    "    my_transforms.Resize(size=256),\n",
    "    my_transforms.FiveCrop(size=224),\n",
    "    my_transforms.ToDType(dtype=torch.float32, scale=True),\n",
    "    my_transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    my_transforms.ConvertTCHWToCTHW(lead_dims=2),\n",
    "    my_transforms.PackInputs(preserved_meta=[]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51af24d4-6fc0-4b85-8fef-741027f85b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 5, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_clips = []\n",
    "for i, batch in enumerate(video2batched(video_ex)):\n",
    "    batch = clip_pipe(batch)\n",
    "    batched_clips.append(batch[\"inputs\"])\n",
    "batched_clips = torch.concatenate(batched_clips, axis=0)\n",
    "batched_clips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca49f4ca-bda1-49ce-8160-62c7ac6063ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 5, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_clips2 = []\n",
    "for i, batch in enumerate(video2batched2(video_ex)):\n",
    "    batch = clip_pipe(batch)\n",
    "    batched_clips2.append(batch[\"inputs\"])\n",
    "batched_clips2 = torch.concatenate(batched_clips2, axis=0)\n",
    "batched_clips2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca48ee5f-e382-4969-a471-7eb25bb5c345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_clips = video2full(video_ex)\n",
    "full_clips = clip_pipe(full_clips)\n",
    "full_clips = full_clips[\"inputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "737aeedc-c048-463f-8f51-09777d7f7929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(batched_clips, batched_clips2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df5a74d0-8d4e-4fec-a1f2-ce8d832b84f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(batched_clips, full_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaaacd98-d22b-4977-865a-5d54e22bb850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(batched_clips2, full_clips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257da55-b085-4c81-8034-c74c642449e2",
   "metadata": {},
   "source": [
    "Preprocessing pipeline is fine, different batch sizes or taking full video yield the same preprocessed clip tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea644c-b86a-44bb-8e58-abc737613a54",
   "metadata": {},
   "source": [
    "## 2. Second, investigate the equality of feature extraction results with different batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1af708-ae39-48d5-b0f1-a16dcdce7261",
   "metadata": {},
   "source": [
    "### I3D ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a0feebe-d9e6-478d-b3e5-08ab1e5f3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_bs2 = \"C:/Users/Jia Herng/Documents/Jia Herng's Docs/Final Year Project/inappropriate-video-detection/feature-extractor/data/outputs/i3d_rgb/1-1004/A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A_bs2.npy\"\n",
    "feat_bs4 = \"C:/Users/Jia Herng/Documents/Jia Herng's Docs/Final Year Project/inappropriate-video-detection/feature-extractor/data/outputs/i3d_rgb/1-1004/A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A_bs4.npy\"\n",
    "feat_full = \"C:/Users/Jia Herng/Documents/Jia Herng's Docs/Final Year Project/inappropriate-video-detection/feature-extractor/data/outputs/i3d_rgb/1-1004/A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A_full.npy\"\n",
    "\n",
    "feat_bs2 = np.load(feat_bs2)\n",
    "feat_bs4 = np.load(feat_bs4)\n",
    "feat_full = np.load(feat_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07700063-8df5-463e-825f-b10ba0c95e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 5, 2048), (24, 5, 2048), (24, 5, 2048))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_bs2.shape, feat_bs4.shape, feat_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5b1c1f6-c436-42c2-b542-b833d02cd3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(feat_bs2, feat_bs4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2146b060-71cd-4077-9ab4-a882b861720a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(feat_bs2, feat_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f91a39a0-d04c-459e-ab4b-e85243786e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(feat_bs4, feat_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e37ef538-5efd-4a40-a339-70935312439e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19795631, 0.21573675, 0.21764307, 0.19901808, 0.17502332,\n",
       "       0.15444714, 0.17563944, 0.16240397, 0.16609856, 0.14471725,\n",
       "       0.14453971, 0.1539778 , 0.16903806, 0.16243568, 0.1976554 ,\n",
       "       0.17051291, 0.13260107, 0.17505392, 0.17537233, 0.19241813,\n",
       "       0.17840326, 0.16508189, 0.19380756, 0.15410993], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(feat_bs2, axis=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85a79210-fbca-49d9-97e2-a55f20136115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19796138, 0.21573432, 0.21764112, 0.19902506, 0.1750286 ,\n",
       "       0.15444554, 0.17565452, 0.16239896, 0.1660938 , 0.1447154 ,\n",
       "       0.14454141, 0.1539816 , 0.16903618, 0.16243584, 0.19766326,\n",
       "       0.17051908, 0.1326073 , 0.17505176, 0.1753808 , 0.19242367,\n",
       "       0.17840007, 0.16507807, 0.19379964, 0.1541156 ], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(feat_bs4, axis=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64716acd-12f1-4f89-95fc-da4c066c11e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19797131, 0.21573214, 0.21764302, 0.199029  , 0.17502555,\n",
       "       0.15444723, 0.17565158, 0.16239482, 0.16609915, 0.1447101 ,\n",
       "       0.1445435 , 0.153988  , 0.16903202, 0.16242996, 0.19766541,\n",
       "       0.17051853, 0.13261503, 0.17504542, 0.1753847 , 0.1924298 ,\n",
       "       0.17839812, 0.16507958, 0.19380033, 0.15411422], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(feat_full, axis=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05117f-3a43-4faa-9b02-0afcb418aa5f",
   "metadata": {},
   "source": [
    "### Video Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5d9a87b-2067-4579-9065-1ea2dbe8292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_bs2 = \"C:/Users/Jia Herng/Documents/Jia Herng's Docs/Final Year Project/inappropriate-video-detection/feature-extractor/data/outputs/swin_rgb/1-1004/A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A_bs2.npy\"\n",
    "feat_bs4 = \"C:/Users/Jia Herng/Documents/Jia Herng's Docs/Final Year Project/inappropriate-video-detection/feature-extractor/data/outputs/swin_rgb/1-1004/A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A_bs4.npy\"\n",
    "feat_full = \"C:/Users/Jia Herng/Documents/Jia Herng's Docs/Final Year Project/inappropriate-video-detection/feature-extractor/data/outputs/swin_rgb/1-1004/A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A_full.npy\"\n",
    "\n",
    "feat_bs2 = np.load(feat_bs2)\n",
    "feat_bs4 = np.load(feat_bs4)\n",
    "feat_full = np.load(feat_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60105871-d85f-41a8-880b-23a1a5bb2e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 5, 768), (24, 5, 768), (24, 5, 768))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_bs2.shape, feat_bs4.shape, feat_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42bbbdef-2553-4224-9eb9-a381c36ffab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(feat_bs2, feat_bs4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c796b19-bc1c-47c6-8c9d-735b53b6f0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(feat_bs2, feat_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18424441-2ead-4317-af6d-d32f3d005d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(feat_bs4, feat_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "997a117b-2ecb-4e2e-aa91-ebf4109cdf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.0272525e-04, -8.3369197e-04, -1.1103182e-03, -2.3060443e-03,\n",
       "        5.6516490e-04,  1.6528585e-03,  9.5496030e-04,  1.0848107e-03,\n",
       "        4.3409329e-04,  1.4218927e-03,  1.9406013e-03,  1.8147645e-03,\n",
       "        1.7271914e-03,  1.6759173e-03, -6.1772135e-03, -5.9339139e-03,\n",
       "        4.7014232e-04, -1.9616617e-03, -9.9494867e-04, -1.4083986e-03,\n",
       "       -9.2717784e-04, -1.6547306e-03, -3.1557010e-04,  8.8795396e-06],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(feat_bs2, axis=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51c0a5e7-9f2a-4838-868a-6c564c4b92f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.0272490e-04, -8.3369302e-04, -1.1103185e-03, -2.3060446e-03,\n",
       "        5.6516513e-04,  1.6528572e-03,  9.5496158e-04,  1.0848098e-03,\n",
       "        4.3409417e-04,  1.4218924e-03,  1.9406017e-03,  1.8147627e-03,\n",
       "        1.7271928e-03,  1.6759166e-03, -6.1772121e-03, -5.9339129e-03,\n",
       "        4.7014357e-04, -1.9616624e-03, -9.9494832e-04, -1.4083990e-03,\n",
       "       -9.2717691e-04, -1.6547312e-03, -3.1556984e-04,  8.8777706e-06],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(feat_bs4, axis=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a2089f5-7ea7-4047-b889-c30f4129867f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.0272438e-04, -8.3369290e-04, -1.1103185e-03, -2.3060450e-03,\n",
       "        5.6516536e-04,  1.6528580e-03,  9.5496140e-04,  1.0848118e-03,\n",
       "        4.3409373e-04,  1.4218917e-03,  1.9406022e-03,  1.8147638e-03,\n",
       "        1.7271922e-03,  1.6759170e-03, -6.1772135e-03, -5.9339125e-03,\n",
       "        4.7014226e-04, -1.9616617e-03, -9.9494832e-04, -1.4084001e-03,\n",
       "       -9.2717673e-04, -1.6547305e-03, -3.1556922e-04,  8.8783290e-06],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(feat_full, axis=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75786422-6d54-4e9a-9e17-bd039ffafb43",
   "metadata": {},
   "source": [
    "batch normalization at inference time (divide by batch size?) \n",
    "- does not affect the slight different in computed feature values. Sample batch mean and sample batch variance are estimated at training time and being used at test time the same way\n",
    "- hypothesis: the inherent randomness in the network itself, we can't expect it to produce the exact same feature even with the same input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895187b-02f1-4cb5-bac2-e067509a702d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-env",
   "language": "python",
   "name": "fyp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
